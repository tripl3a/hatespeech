{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Classification on Hate Speech\n",
    "\n",
    "---\n",
    "\n",
    "__2nd Semester Data Science Master__  \n",
    "__Beuth University of Applied Sciences Berlin__\n",
    "\n",
    "__by Arndt, Ana, Christian, Ervin, Malte__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Content\n",
    "--- \n",
    "\n",
    "1. Preprocessing\n",
    "1. Baseline\n",
    "1. Improvements\n",
    "1. Ensembles\n",
    "1. Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import sklearn.metrics as skm\n",
    "from os import chdir\n",
    "from sklearn import model_selection\n",
    "import skift\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "chdir(\"/home/arndt/git-reps/hatespeech/\")\n",
    "\n",
    "def load_train_test_data(train, test, test_labels, classname):\n",
    "\n",
    "    df_test = pd.merge(pd.read_csv(test),\n",
    "                       pd.read_csv(test_labels),\n",
    "                       how=\"inner\",\n",
    "                       on=\"id\")\n",
    "\n",
    "    df_train=pd.read_csv(train)\n",
    "\n",
    "    # data preperation\n",
    "\n",
    "    df_train=df_train[[\"id\",\"comment_text\",classname]]\n",
    "    df_train[\"comment_text\"]=df_train[\"comment_text\"].apply(str.replace,args=(\"\\n\",\" \"))\n",
    "    df_train[\"comment_text\"]=df_train[\"comment_text\"].apply(str.replace,args=(\"\\\"\",\"\"))\n",
    "\n",
    "    df_test[\"comment_text\"]=df_test[\"comment_text\"].apply(str.replace,args=(\"\\n\",\" \"))\n",
    "    df_test[\"comment_text\"]=df_test[\"comment_text\"].apply(str.replace,args=(\"\\\"\",\"\"))\n",
    "\n",
    "    X_train = pd.DataFrame(df_train.loc[:,\"comment_text\"])\n",
    "    y_train = df_train.loc[:,classname]\n",
    "    #-1 means the row wasn't used for scoring in the Kaggle competition\n",
    "    X_test = pd.DataFrame(df_test[df_test[classname]>-1].loc[:,\"comment_text\"]) \n",
    "    y_test = df_test[df_test[classname]>-1].loc[:,classname]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def score_preds(y_true, y_pred):\n",
    "    print(\"confusion matrix:\")\n",
    "    print(str(skm.confusion_matrix(y_true, y_pred)))\n",
    "    print(\"classification report:\")\n",
    "    print(str(skm.classification_report(y_true, y_pred)))\n",
    "    print(\"f1 macro: %0.4f\" % (skm.precision_recall_fscore_support(y_true, y_pred, average='macro')[2]))\n",
    "    print(\"f1 micro: %0.4f\" % (skm.precision_recall_fscore_support(y_true, y_pred, average='micro')[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Preprocessing and Normalization  \n",
    "---  \n",
    "Improve the performance of the model applying some simple pre-processing  \n",
    "- Translation into English \n",
    "- Only ASCII characters (unidecode)\n",
    "- Remove special characters \n",
    "- Change Emojis to words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Google Translation API Requests\n",
    "---\n",
    "\n",
    "![Dashboard](images/google_dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Replace Characters  \n",
    "---  \n",
    "```py \n",
    "def replacetext(text):\n",
    "    for key, value in REPLACE_TO.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text\n",
    "```\n",
    "\n",
    "```py \n",
    "REPLACE_TO = {\n",
    "   ':)': 'happy',':(': 'sad',':P': 'funny','@': 'at','&': 'and','i\\'m': 'i am','don\\'t': 'do not','can\\'t': 'can not',\n",
    "   '.': ' ',',': ' ',':': ' ', ';': ' ','!': ' ','\\'': ' ','?': ' ', '(': ' ', ')': ' ', '[': ' ', ']': ' ', '-': ' ',\n",
    "   '#': ' ', '=': ' ', '+': ' ', '/': ' ', '\"': ' ', '0': ' zero ', '1': ' one ', '2': ' two ', '3': ' three ', '3': ' three ',\n",
    "   '4': ' four ','5': ' five ', '6': ' six ', '7': ' seven ', '8': ' eight ', '9': ' nine ',\n",
    "   '|':' ', '$':' ', '%':' ', '^':' ', '*':' ', '_':' ', '{':' ', '}':' ', '<':' ', '>':' ',\n",
    "   '\\n': ' '\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Majority class classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[57888     0]\n",
      " [ 6090     0]]\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.95     57888\n",
      "          1       0.00      0.00      0.00      6090\n",
      "\n",
      "avg / total       0.82      0.90      0.86     63978\n",
      "\n",
      "f1 macro: 0.4750\n",
      "f1 micro: 0.9048\n"
     ]
    }
   ],
   "source": [
    "score_preds(y_test, np.zeros(y_test.shape)) #set all predictions to non-toxic (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* By only assigning all fitted values to the majority class we get a F1 score of 90%. \n",
    "* This is, because the test dataset is imbalanced and contains only 10% toxic comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Single model - baseline\n",
    "\n",
    "using skift (scikit fasttext) - scikit-learn wrappers for Python ([GitHub](https://github.com/shaypal5/skift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[54325  3563]\n",
      " [ 1218  4872]]\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.94      0.96     57888\n",
      "          1       0.58      0.80      0.67      6090\n",
      "\n",
      "avg / total       0.94      0.93      0.93     63978\n",
      "\n",
      "f1 macro: 0.8143\n",
      "f1 micro: 0.9253\n",
      "f1 micro on training data: 0.9722\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_train_test_data(\"data/train.csv\", \"data/test.csv\", \"data/test_labels.csv\", \"toxic\")\n",
    "\n",
    "skift_clf = skift.FirstObjFtClassifier()\n",
    "skift_clf.fit(X_train, y_train)\n",
    "preds = skift_clf.predict(X_test)\n",
    "score_preds(y_test, preds)\n",
    "print(\"f1 micro on training data: %0.4f\" % (skift_clf.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Single model - preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[54324  3564]\n",
      " [ 1222  4868]]\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.94      0.96     57888\n",
      "          1       0.58      0.80      0.67      6090\n",
      "\n",
      "avg / total       0.94      0.93      0.93     63978\n",
      "\n",
      "f1 macro: 0.8141\n",
      "f1 micro: 0.9252\n",
      "f1 micro on training data: 0.9722\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_train_test_data(\"data/train_unidecode.csv\", \"data/test_unidecode.csv\", \"data/test_labels.csv\", \"toxic\")\n",
    "\n",
    "skift_clf = skift.FirstObjFtClassifier()\n",
    "skift_clf.fit(X_train, y_train)\n",
    "preds = skift_clf.predict(X_test)\n",
    "score_preds(y_test, preds)\n",
    "print(\"f1 micro on training data: %0.4f\" % (skift_clf.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Single model - pretrained vectors\n",
    "\n",
    "fastText English Word Vectors trained on Wikipedia 2017, UMBC webbase corpus, and statmt.org\n",
    "\n",
    "** there is an issue with the pretrainedVectors parameter, it is beeing ignored **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[54738  3150]\n",
      " [ 1495  4595]]\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.95      0.96     57888\n",
      "          1       0.59      0.75      0.66      6090\n",
      "\n",
      "avg / total       0.94      0.93      0.93     63978\n",
      "\n",
      "f1 macro: 0.8118\n",
      "f1 micro: 0.9274\n",
      "f1 micro on training data: 0.9684\n"
     ]
    }
   ],
   "source": [
    "skift_clf = skift.FirstObjFtClassifier(minn=3, maxn=3, pretrainedVectors=\"vectors/wiki-news-300d-1M-subword.vec\")\n",
    "skift_clf.fit(X_train, y_train)\n",
    "preds = skift_clf.predict(X_test)\n",
    "score_preds(y_test, preds)\n",
    "print(\"f1 micro on training data: %0.4f\" % (skift_clf.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Single model - parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[55608  2280]\n",
      " [ 1939  4151]]\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.96      0.96     57888\n",
      "          1       0.65      0.68      0.66      6090\n",
      "\n",
      "avg / total       0.94      0.93      0.93     63978\n",
      "\n",
      "f1 macro: 0.8132\n",
      "f1 micro: 0.9341\n",
      "f1 micro on training data: 0.9586\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_train_test_data(\"data/train_unidecode.csv\", \"data/test_unidecode.csv\", \"data/test_labels.csv\", \"toxic\")\n",
    "\n",
    "skift_clf = skift.FirstObjFtClassifier(wordNgrams=2, maxn=3, dim=300)\n",
    "skift_clf.fit(X_train, y_train)\n",
    "preds = skift_clf.predict(X_test)\n",
    "score_preds(y_test, preds)\n",
    "print(\"f1 micro on training data: %0.4f\" % (skift_clf.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733432851735352\n",
      "0.9461546434494196\n",
      "0.7536945812807881\n",
      "0.5955624756714675\n"
     ]
    }
   ],
   "source": [
    "### For non-toxic ###\n",
    "# Recall = TP/(TP+FN)\n",
    "print(54771/(54771+1500))\n",
    "# Precision = TP/(TP+FP)\n",
    "print(54771/(54771+3117))\n",
    "\n",
    "### For toxic ###\n",
    "# Recall = TP/(TP+FN)\n",
    "print(4590/(4590+1500))\n",
    "# Precision = TP/(TP+FP)\n",
    "print(4590/(4590+3117))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Check common errors - false negatives\n",
    "\n",
    "What commonnalities have the false negatives? Check common errors …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "comp = pd.merge(pd.DataFrame({\"comment_text\" : X_test[\"comment_text\"].values, \"toxic_pred\" : preds}),\n",
    "                pd.concat([X_test, y_test], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1218, 3)\n",
      "                                          comment_text  toxic_pred  toxic\n",
      "8       arabs are committing genocide in iraq  but ...         0.0      1\n",
      "106      well  it sucks to have a university to be ...         0.0      1\n",
      "133       so  on the tenth anniversary of  nine   o...         0.0      1\n",
      "195           hey shithead  stop vandilizing articles          0.0      1\n",
      "279                   karl tearle is a mop haired twat         0.0      1\n"
     ]
    }
   ],
   "source": [
    "false_negatives = comp[(comp[\"toxic\"]==1) & (comp[\"toxic_pred\"]==0)]\n",
    "print(false_negatives.shape)\n",
    "false_negatives[\"comment_text\"].to_csv(\"data/false_negatives.txt\")\n",
    "print(false_negatives.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## False negatives: labeled as toxic, not identified as toxic\n",
    "\n",
    "- \"well  it **sucks** to have a university to be nicknameless  and it s the first time in ncaa history that it has happened\"\n",
    "- \"   intolerance in india        india is a generator of liars like you\"\n",
    "- \"look  you re a pedant  and fetzer is a jew hater  on press tv in the uk today   september  two nd  two  zero  one  one    he said that the israelis were behind  nine   one  one   the man is a complete fool \"\n",
    "- \"not even every sexual person fantasizes while masturbating  most males do  but many females do not   i think most libidinous asexuals masturbate for the same reason they would scratch themselves if they were itchy \"\n",
    "\n",
    "\n",
    "- **few or no predominantly abusive-use words in a \"normal speech context\"**\n",
    "- **many would actually not label these comments as toxic**\n",
    "- **could argue that the classifier actually does a good job**\n",
    "\n",
    "### but:\n",
    "- \"hey **shithead**  stop vandilizing articles \" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Check common errors - false positives\n",
    "\n",
    "What commonnalities have the false positives? Check common errors …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3563, 3)\n",
      "                                         comment_text  toxic_pred  toxic\n",
      "1                     dear god this site is horrible          1.0      0\n",
      "27  i will burn you to hell if you revoke my talk ...         1.0      0\n",
      "78          shameless canvass       hello  diannaa...         1.0      0\n",
      "79                          what the hell      justin         1.0      0\n",
      "87      buffoon synonyms     bozo  buffo  clown  c...         1.0      0\n"
     ]
    }
   ],
   "source": [
    "false_positives = comp[(comp[\"toxic\"]==0) & (comp[\"toxic_pred\"]==1)]\n",
    "print(false_positives.shape)\n",
    "false_positives[\"comment_text\"].to_csv(\"data/false_positives.txt\")\n",
    "print(false_positives.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## False Positives: labeled as non-toxic, identified as toxic\n",
    "\n",
    "- \"i will burn you to hell if you revoke my talk page access\" - **wrongly labeled**\n",
    "- \"  buffoon synonyms     bozo  buffo  clown  comedian  comic  fool  harlequin  humorist  idiot  jerk  jester  joker  merry andrew  mime  mimic  mummer  playboy  prankster  ridicule  stooge  wag  wit  zany \" - **special context**\n",
    "- \" gay       he s gay too  it should be noted that he has a male partner \" **non-abusive use of a term that is predominantly used in an abusive way in the corpus** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building fastText Ensembles\n",
    "---\n",
    "\n",
    "Make predictions with a collection of classifiers on a dataset X.\n",
    "\n",
    "* K Fold\n",
    "* Stratified K Fold\n",
    "* Bagging\n",
    "* Bagging with Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def ensemble_predict_proba(classifiers, X):\n",
    "    proba = [classifier.predict_proba(X) for classifier in classifiers]\n",
    "    mean = np.zeros(proba[0].shape)\n",
    "    for i in range(len(classifiers)):\n",
    "        mean = mean + proba[i]\n",
    "    mean = mean / float(len(classifiers))\n",
    "    return mean\n",
    "\n",
    "def ensemble_predict(classifiers, X):\n",
    "    kfold_proba = ensemble_predict_proba(classifiers, X)\n",
    "    kfold_labels = np.zeros(kfold_proba.shape[0]) #initialize array\n",
    "    kfold_labels[kfold_proba[:,0]<=kfold_proba[:,1]] = 1\n",
    "    return kfold_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Folds\n",
    "---\n",
    "\n",
    "![KFold](images/kfold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Build multiple models using K-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on test proportion of this fold: 0.961\n",
      "Score on test proportion of this fold: 0.962\n",
      "Score on test proportion of this fold: 0.958\n",
      "Score on test proportion of this fold: 0.958\n",
      "Score on test proportion of this fold: 0.961\n",
      "Score on test proportion of this fold: 0.960\n",
      "Score on test proportion of this fold: 0.960\n",
      "Score on test proportion of this fold: 0.961\n",
      "Score on test proportion of this fold: 0.961\n",
      "Score on test proportion of this fold: 0.960\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_train_test_data(\"data/train_unidecode.csv\", \"data/test_unidecode.csv\", \"data/test_labels.csv\", \"toxic\")\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, shuffle=True) #add variance through randomnization\n",
    "\n",
    "# build multiple models using k folds:\n",
    "kfold_clfs = list()\n",
    "for train_index, test_index in kfold.split(X_train):\n",
    "    clf = skift.FirstObjFtClassifier(minn=3, maxn=3, pretrainedVectors=\"data/wiki-news-300d-1M.vec\")\n",
    "    clf.fit(X_train.iloc[train_index], y_train.iloc[train_index])\n",
    "    print(\"Score on test proportion of this fold: %0.3f\" % (clf.score(X_train.iloc[test_index], y_train.iloc[test_index])))\n",
    "    kfold_clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[54887  3001]\n",
      " [ 1431  4659]]\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.95      0.96     57888\n",
      "          1       0.61      0.77      0.68      6090\n",
      "\n",
      "avg / total       0.94      0.93      0.93     63978\n",
      "\n",
      "f1 macro: 0.8194\n",
      "f1 micro: 0.9307\n"
     ]
    }
   ],
   "source": [
    "score_preds(y_test, ensemble_predict(kfold_clfs, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# StratifiedKFold\n",
    "\n",
    "* There are different strategies in creating a train set and test set split of your data. \n",
    "* If you want to keep the percentage for each class in each fold the same you want to use a stratified split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on test proportion of this fold: 0.959\n",
      "Score on test proportion of this fold: 0.960\n",
      "Score on test proportion of this fold: 0.961\n",
      "Score on test proportion of this fold: 0.961\n",
      "Score on test proportion of this fold: 0.959\n",
      "Score on test proportion of this fold: 0.961\n",
      "Score on test proportion of this fold: 0.960\n",
      "Score on test proportion of this fold: 0.960\n",
      "Score on test proportion of this fold: 0.962\n",
      "Score on test proportion of this fold: 0.963\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_train_test_data(\"data/train_unidecode.csv\", \"data/test_unidecode.csv\", \"data/test_labels.csv\", \"toxic\")\n",
    "\n",
    "stkfold = model_selection.StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# build multiple models using k folds:\n",
    "stkfold_clfs = list()\n",
    "for train_index, test_index in stkfold.split(X=X_train, y=y_train):\n",
    "    clf = skift.FirstObjFtClassifier(minn=3, maxn=3, pretrainedVectors=\"data/wiki-news-300d-1M.vec\")\n",
    "    clf.fit(X_train.iloc[train_index], y_train.iloc[train_index])\n",
    "    print(\"Score on test proportion of this fold: %0.3f\" % (clf.score(X_train.iloc[test_index], y_train.iloc[test_index])))\n",
    "    stkfold_clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[54915  2973]\n",
      " [ 1436  4654]]\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.95      0.96     57888\n",
      "          1       0.61      0.76      0.68      6090\n",
      "\n",
      "avg / total       0.94      0.93      0.93     63978\n",
      "\n",
      "f1 macro: 0.8200\n",
      "f1 micro: 0.9311\n"
     ]
    }
   ],
   "source": [
    "score_preds(y_test, ensemble_predict(stkfold_clfs, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## KFold Conclusion\n",
    "\n",
    "The main reason we started to use KFold was that we didn't have the labeled test data at the beginning. But after we found the real test data on Kaggle, we used it.\n",
    "\n",
    "* k=10 slightly improved the score on the test set\n",
    "* k=5 scored worse than just a single model on all the training data \n",
    "* StratifiedKFold performed worse than the just KFold.\n",
    "\n",
    "I would not balance the data within the folds, as the data will not be balanced in a real-world example. Thus, the cross-validation score will not be represent the model performance well.\n",
    "\n",
    "Some ways to deal with imbalanced data is under- and over-sampling (e.g. SMOTE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Bagging](images/bagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Oversampling the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def oversample(X, y, p_oversample_size, p_oversample_ratio):\n",
    "    y_true_idx = y[y==1].index\n",
    "    y_false_idx = y[y==0].index\n",
    "    \n",
    "    true_frac = float(X.loc[y_true_idx,].count() / X.count())\n",
    "    false_frac = float(X.loc[y_false_idx,].count() / X.count())\n",
    "    oversample_true_frac = p_oversample_size * p_oversample_ratio / true_frac\n",
    "    oversample_false_frac = p_oversample_size * (1-p_oversample_ratio) / false_frac \n",
    "    \n",
    "    X_true =  X.loc[y_true_idx,].sample(frac=oversample_true_frac, replace=True)\n",
    "    X_false =  X.loc[y_false_idx,].sample(frac=oversample_false_frac, replace=True)\n",
    "\n",
    "    X_resampled = pd.concat([X_true, X_false])\n",
    "    y_resampled = y.loc[X_resampled.index]\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(sorted(Counter(y_train).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Bagging ensemble - Oversampling w/ seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oversampling fraction: 0.0800 // score: 0.9342\n",
      "oversampling fraction: 0.0825 // score: 0.9351\n",
      "oversampling fraction: 0.0850 // score: 0.9233\n",
      "oversampling fraction: 0.0875 // score: 0.9355\n",
      "oversampling fraction: 0.0900 // score: 0.9357\n",
      "oversampling fraction: 0.0925 // score: 0.9353\n",
      "oversampling fraction: 0.0950 // score: 0.9333\n",
      "oversampling fraction: 0.0975 // score: 0.9141\n",
      "oversampling fraction: 0.1000 // score: 0.9354\n",
      "oversampling fraction: 0.1025 // score: 0.9192\n",
      "oversampling fraction: 0.1050 // score: 0.9235\n",
      "oversampling fraction: 0.1075 // score: 0.9196\n",
      "confusion matrix:\n",
      "[[56181  1707]\n",
      " [ 2477  3613]]\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.97      0.96     57888\n",
      "          1       0.68      0.59      0.63      6090\n",
      "\n",
      "avg / total       0.93      0.93      0.93     63978\n",
      "\n",
      "f1 macro: 0.7987\n",
      "f1 micro: 0.9346\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_train_test_data(\"data/train_unidecode.csv\", \"data/test_unidecode.csv\", \"data/test_labels.csv\", \"toxic\")\n",
    "\n",
    "# Oversampling different fractions and score\n",
    "oversample_clfs = list()\n",
    "for f in list(np.arange(0.08, 0.11, 0.0025)):\n",
    "    X_resampled, y_resampled = oversample(X_train, y_train, 1.25, f, seed)\n",
    "    skift_clf = skift.FirstObjFtClassifier(lr=0.2)\n",
    "    skift_clf.fit(X_resampled, y_resampled)\n",
    "    print(\"oversampling fraction: %0.4f // score: %0.4f\" % (f, skift_clf.score(X_test, y_test)))\n",
    "    oversample_clfs.append(skift_clf)\n",
    "\n",
    "score_preds(y_test, ensemble_predict(oversample_clfs, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Bagging ensemble - Oversampling w/o seed - 100 bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oversampling fraction: 0.0800 // score: 0.9200\n",
      "oversampling fraction: 0.0825 // score: 0.9362\n",
      "oversampling fraction: 0.0850 // score: 0.9376\n",
      "oversampling fraction: 0.0875 // score: 0.9179\n",
      "oversampling fraction: 0.0900 // score: 0.9224\n",
      "oversampling fraction: 0.0925 // score: 0.9356\n",
      "oversampling fraction: 0.0950 // score: 0.9195\n",
      "oversampling fraction: 0.0975 // score: 0.9128\n",
      "oversampling fraction: 0.1000 // score: 0.9161\n",
      "oversampling fraction: 0.1025 // score: 0.9117\n",
      "oversampling fraction: 0.1050 // score: 0.9194\n",
      "oversampling fraction: 0.1075 // score: 0.9133\n",
      "oversampling fraction: 0.0800 // score: 0.9364\n",
      "oversampling fraction: 0.0825 // score: 0.9238\n",
      "oversampling fraction: 0.0850 // score: 0.9172\n",
      "oversampling fraction: 0.0875 // score: 0.9376\n",
      "oversampling fraction: 0.0900 // score: 0.9367\n",
      "oversampling fraction: 0.0925 // score: 0.9196\n",
      "oversampling fraction: 0.0950 // score: 0.9259\n",
      "oversampling fraction: 0.0975 // score: 0.9382\n",
      "oversampling fraction: 0.1000 // score: 0.9381\n",
      "oversampling fraction: 0.1025 // score: 0.9137\n",
      "oversampling fraction: 0.1050 // score: 0.9241\n",
      "oversampling fraction: 0.1075 // score: 0.9206\n",
      "oversampling fraction: 0.0800 // score: 0.9347\n",
      "oversampling fraction: 0.0825 // score: 0.9215\n",
      "oversampling fraction: 0.0850 // score: 0.9257\n",
      "oversampling fraction: 0.0875 // score: 0.9255\n",
      "oversampling fraction: 0.0900 // score: 0.9243\n",
      "oversampling fraction: 0.0925 // score: 0.9310\n",
      "oversampling fraction: 0.0950 // score: 0.9226\n",
      "oversampling fraction: 0.0975 // score: 0.9138\n",
      "oversampling fraction: 0.1000 // score: 0.9229\n",
      "oversampling fraction: 0.1025 // score: 0.9106\n",
      "oversampling fraction: 0.1050 // score: 0.9162\n",
      "oversampling fraction: 0.1075 // score: 0.9215\n",
      "oversampling fraction: 0.0800 // score: 0.9119\n",
      "oversampling fraction: 0.0825 // score: 0.9374\n",
      "oversampling fraction: 0.0850 // score: 0.9239\n",
      "oversampling fraction: 0.0875 // score: 0.9325\n",
      "oversampling fraction: 0.0900 // score: 0.9374\n",
      "oversampling fraction: 0.0925 // score: 0.9375\n",
      "oversampling fraction: 0.0950 // score: 0.9146\n",
      "oversampling fraction: 0.0975 // score: 0.9094\n",
      "oversampling fraction: 0.1000 // score: 0.9016\n",
      "oversampling fraction: 0.1025 // score: 0.9235\n",
      "oversampling fraction: 0.1050 // score: 0.9227\n",
      "oversampling fraction: 0.1075 // score: 0.9346\n",
      "oversampling fraction: 0.0800 // score: 0.9365\n",
      "oversampling fraction: 0.0825 // score: 0.9361\n",
      "oversampling fraction: 0.0850 // score: 0.9082\n",
      "oversampling fraction: 0.0875 // score: 0.9364\n",
      "oversampling fraction: 0.0900 // score: 0.9373\n",
      "oversampling fraction: 0.0925 // score: 0.9367\n",
      "oversampling fraction: 0.0950 // score: 0.9204\n",
      "oversampling fraction: 0.0975 // score: 0.9077\n",
      "oversampling fraction: 0.1000 // score: 0.9181\n",
      "oversampling fraction: 0.1025 // score: 0.9236\n",
      "oversampling fraction: 0.1050 // score: 0.9374\n",
      "oversampling fraction: 0.1075 // score: 0.9128\n",
      "oversampling fraction: 0.0800 // score: 0.9365\n",
      "oversampling fraction: 0.0825 // score: 0.9201\n",
      "oversampling fraction: 0.0850 // score: 0.9344\n",
      "oversampling fraction: 0.0875 // score: 0.9163\n",
      "oversampling fraction: 0.0900 // score: 0.9370\n",
      "oversampling fraction: 0.0925 // score: 0.9209\n",
      "oversampling fraction: 0.0950 // score: 0.9133\n",
      "oversampling fraction: 0.0975 // score: 0.9264\n",
      "oversampling fraction: 0.1000 // score: 0.9254\n",
      "oversampling fraction: 0.1025 // score: 0.9127\n",
      "oversampling fraction: 0.1050 // score: 0.9118\n",
      "oversampling fraction: 0.1075 // score: 0.9366\n",
      "oversampling fraction: 0.0800 // score: 0.9373\n",
      "oversampling fraction: 0.0825 // score: 0.9230\n",
      "oversampling fraction: 0.0850 // score: 0.9371\n",
      "oversampling fraction: 0.0875 // score: 0.9203\n",
      "oversampling fraction: 0.0900 // score: 0.9240\n",
      "oversampling fraction: 0.0925 // score: 0.9349\n",
      "oversampling fraction: 0.0950 // score: 0.9214\n",
      "oversampling fraction: 0.0975 // score: 0.9253\n",
      "oversampling fraction: 0.1000 // score: 0.9302\n",
      "oversampling fraction: 0.1025 // score: 0.9339\n",
      "oversampling fraction: 0.1050 // score: 0.9218\n",
      "oversampling fraction: 0.1075 // score: 0.9145\n",
      "oversampling fraction: 0.0800 // score: 0.9114\n",
      "oversampling fraction: 0.0825 // score: 0.9384\n",
      "oversampling fraction: 0.0850 // score: 0.9218\n",
      "oversampling fraction: 0.0875 // score: 0.9148\n",
      "oversampling fraction: 0.0900 // score: 0.9287\n",
      "oversampling fraction: 0.0925 // score: 0.9238\n",
      "oversampling fraction: 0.0950 // score: 0.9152\n",
      "oversampling fraction: 0.0975 // score: 0.9250\n",
      "oversampling fraction: 0.1000 // score: 0.9115\n",
      "oversampling fraction: 0.1025 // score: 0.9342\n",
      "oversampling fraction: 0.1050 // score: 0.9286\n",
      "oversampling fraction: 0.1075 // score: 0.9162\n",
      "confusion matrix:\n",
      "[[54913  2975]\n",
      " [ 1444  4646]]\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.95      0.96     57888\n",
      "          1       0.61      0.76      0.68      6090\n",
      "\n",
      "avg / total       0.94      0.93      0.93     63978\n",
      "\n",
      "f1 macro: 0.8195\n",
      "f1 micro: 0.9309\n"
     ]
    }
   ],
   "source": [
    "# Oversampling different fractions and score\n",
    "\n",
    "oversample_clfs = list()\n",
    "\n",
    "for i in range(8):\n",
    "    for f in list(np.arange(0.08, 0.11, 0.0025)):\n",
    "        X_resampled, y_resampled = oversample(X_train, y_train, 1.25, f)\n",
    "        skift_clf = skift.FirstObjFtClassifier(lr=0.2)\n",
    "        skift_clf.fit(X_resampled, y_resampled)\n",
    "        skift_clf.model.quantize()\n",
    "        print(\"oversampling fraction: %0.4f // score: %0.4f\" % (f, skift_clf.score(X_test, y_test)))\n",
    "        oversample_clfs.append(skift_clf)\n",
    "\n",
    "score_preds(y_test, ensemble_predict(oversample_clfs, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Result comparison - single models\n",
    "**Single model, original data, default parameters**  \n",
    "``\n",
    "[[54325  3563]                        f1 macro: 0.8143  \n",
    " [ 1218  4872]]                       f1 micro: 0.9253  \n",
    "                                      f1 micro on training data: 0.9722  \n",
    "``\n",
    "\n",
    "**Single model, preprocessed data, default parameters**  \n",
    "``\n",
    "[[54324  3564]                        f1 macro: 0.8141  \n",
    " [ 1222  4868]]                       f1 micro: 0.9252  \n",
    "                                      f1 micro on training data: 0.9722  \n",
    "``\n",
    "\n",
    "**Single model, preprocessed data, wordNgrams=2, maxn=3, dim=300**  \n",
    "``\n",
    "[[55608  2280]                        f1 macro: 0.8132  \n",
    " [ 1939  4151]]                       f1 micro: 0.9341  \n",
    "                                      f1 micro on training data: 0.9586  \n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Result comparison - ensembles\n",
    "**10 Folds, preprocessed data, minn=3, maxn=3, wiki-news-300d-1M-subword.vec**  \n",
    "``\n",
    "[[54887  3001]                        f1 macro: 0.8194  \n",
    " [ 1431  4659]]                       f1 micro: 0.9307  \n",
    "``\n",
    "\n",
    "**10 Stratified Folds, preprocessed data, minn=3, maxn=3, wiki-news-300d-1M-subword.vec**  \n",
    "``\n",
    "[[54915  2973]                        f1 macro: 0.8200  \n",
    " [ 1436  4654]]                       f1 micro: 0.9311  \n",
    "``\n",
    "\n",
    "**Oversampling ensemble, preprocessed data, 8 bags, lr=0.2**  \n",
    "``\n",
    "[[56181  1707]                        f1 macro: 0.7987  \n",
    " [ 2477  3613]]                       f1 micro: 0.9346  \n",
    "``\n",
    "\n",
    "**Oversampling ensemble, preprocessed data, 96 bags, lr=0.2**  \n",
    "``\n",
    "[[54913  2975]                        f1 macro: 0.8195  \n",
    " [ 1444  4646]]                       f1 micro: 0.9309  \n",
    "``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Oversampling Ensemble Histogram  \n",
    "---\n",
    "\n",
    "![OversamplingHistogram.png](images/OversamplingHistogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Identity hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[63026   240]\n",
      " [  479   233]]\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     63266\n",
      "          1       0.49      0.33      0.39       712\n",
      "\n",
      "avg / total       0.99      0.99      0.99     63978\n",
      "\n",
      "f1 macro: 0.6938\n",
      "f1 micro: 0.9888\n",
      "f1 micro on training data: 0.9934\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_train_test_data(\"data/train_unidecode.csv\", \"data/test_unidecode.csv\", \"data/test_labels.csv\", \"identity_hate\")\n",
    "\n",
    "skift_clf = skift.FirstObjFtClassifier()\n",
    "skift_clf.fit(X_train, y_train)\n",
    "preds = skift_clf.predict(X_test)\n",
    "score_preds(y_test, preds)\n",
    "print(\"f1 micro on training data: %0.4f\" % (skift_clf.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "* Random results - due to initialization of neural net's weights - make result comparison difficult\n",
    "* Ensembles to stabilize the results\n",
    "* Really unclear on how some parameters improve the score i.e. pretrained vectors\n",
    "* Usage within scikit-learn difficult, if you don't have numeric predictors \n",
    "\n",
    "## More ideas\n",
    "\n",
    "* GridSearch on \"good\" fastText hyperparameters\n",
    "* Generate many models and persist one that scores high\n",
    "* Continousliy improve this persisted model\n",
    "* More detailed comparison of the probabilities of FPs/FNs of different models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tfidf Method using Scikit vectorizer\n",
    "\n",
    "This method was inspired by one of the Kaggle Competitors who used sklearn to implement a Logistic regression with words & char n grams. And his work achieved a better score only to mention that it doesn't use fastText at all for it's implementation.\n",
    "\n",
    "Source: https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams/code \n",
    "\n",
    "A few edits were made to create the following result:\n",
    "* Test CV score (ROC AUC) for class toxic is 0.957\n",
    "* Test CV score (ROC AUC) for class identity_hate is 0.975\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CV score for class toxic is 0.9567386963649188\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import chdir, path\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "chdir(path.dirname(path.abspath('__file__')))\n",
    "train = pd.read_csv('data/train.csv').fillna(' ')\n",
    "test = pd.read_csv('data/test.csv').fillna(' ')\n",
    "\n",
    "df_test = pd.merge(pd.read_csv(\"data/test.csv\"),\n",
    "                   pd.read_csv(\"data/test_labels.csv\"),\n",
    "                   how=\"inner\",\n",
    "                   on=\"id\")\n",
    "\n",
    "#y_test = df_test[df_test[\"toxic\"]>-1].loc[:,\"toxic\"]\n",
    "train_text = train['comment_text']\n",
    "test_text = pd.DataFrame(df_test[df_test[\"toxic\"]>-1].loc[:,\"comment_text\"])[\"comment_text\"]\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=50000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])\n",
    "\n",
    "scores = []\n",
    "scores_test = []\n",
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "for class_name in [\"toxic\"]:\n",
    "\n",
    "    train_target = train[class_name]\n",
    "    test_target = df_test[df_test[class_name]>-1].loc[:,\"toxic\"]\n",
    "\n",
    "    classifier_test = LogisticRegression(C=0.1, solver='sag')\n",
    "    classifier_test.fit(train_features, train_target)\n",
    "\n",
    "    cv_score_test = np.mean(cross_val_score(classifier_test, test_features, test_target, cv=3, scoring='roc_auc'))\n",
    "    scores_test.append(cv_score_test)\n",
    "    print('Test CV score for class {} is {}'.format(class_name, cv_score_test))\n",
    "\n",
    "    #classifier.fit(train_features, train_target)\n",
    "    #submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "\n",
    "#print('Total CV score is {}'.format(np.mean(scores)))\n",
    "#print('Total Test CV score is {}'.format(np.mean(scores_test)))\n",
    "\n",
    "#submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CV score for class identity_hate is 0.9748963999977726\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import chdir, path\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "chdir(path.dirname(path.abspath('__file__')))\n",
    "train = pd.read_csv('data/train.csv').fillna(' ')\n",
    "test = pd.read_csv('data/test.csv').fillna(' ')\n",
    "\n",
    "df_test = pd.merge(pd.read_csv(\"data/test.csv\"),\n",
    "                   pd.read_csv(\"data/test_labels.csv\"),\n",
    "                   how=\"inner\",\n",
    "                   on=\"id\")\n",
    "\n",
    "#y_test = df_test[df_test[\"toxic\"]>-1].loc[:,\"toxic\"]\n",
    "train_text = train['comment_text']\n",
    "test_text = pd.DataFrame(df_test[df_test[\"identity_hate\"]>-1].loc[:,\"comment_text\"])[\"comment_text\"]\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=50000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])\n",
    "\n",
    "scores = []\n",
    "scores_test = []\n",
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "for class_name in [\"identity_hate\"]:\n",
    "\n",
    "    train_target = train[class_name]\n",
    "    test_target = df_test[df_test[class_name]>-1].loc[:,\"identity_hate\"]\n",
    "\n",
    "    classifier_test = LogisticRegression(C=0.1, solver='sag')\n",
    "    classifier_test.fit(train_features, train_target)\n",
    "\n",
    "    cv_score_test = np.mean(cross_val_score(classifier_test, test_features, test_target, cv=3, scoring='roc_auc'))\n",
    "    scores_test.append(cv_score_test)\n",
    "    print('Test CV score for class {} is {}'.format(class_name, cv_score_test))\n",
    "\n",
    "    #classifier.fit(train_features, train_target)\n",
    "    #submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "\n",
    "#print('Total CV score is {}'.format(np.mean(scores)))\n",
    "#print('Total Test CV score is {}'.format(np.mean(scores_test)))\n",
    "\n",
    "#submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Feedback \n",
    "\n",
    "fT by default uses the dataset it sees to build word embeddings  \n",
    "\n",
    "for translation use https://www.linguee.de/, they also made the DeepL Translator  \n",
    "\n",
    "regarding preprocessing: \n",
    "- fT removes punctuation (and thus variance), that's why it's important to e.g. replace smileys by words to keep this variance\n",
    "\n",
    "parameter tuning is good, more importantly further research in this directions:\n",
    "- get complementary data (like the back and forth translations)\n",
    "- create own word/character embeddings on additional corpous (search the web for open data sources)\n",
    "- create silver standard: computational labelling of unlabeled data \n",
    "- feature engineering (less important)\n",
    "\n",
    "it's important to create extra signals by generating or adding additional data "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
